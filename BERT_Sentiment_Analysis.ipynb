{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Latest_Stanford_SentoimentAnal.ipynb","provenance":[{"file_id":"1f_m4Uj0QTZF5GHCJPT2Fm326GqsUQNLa","timestamp":1594852282661}],"collapsed_sections":[],"authorship_tag":"ABX9TyOi+IDcBO13lHOTU9rQV9io"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a8aeb07b4b33433aa170cf7478e1fbae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd0ea0b9634346e380e2888dfd00c942","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_77e86825addb40c9b7968ac4869cb618","IPY_MODEL_783da47933514946a0df4f321a42bfc2"]}},"bd0ea0b9634346e380e2888dfd00c942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77e86825addb40c9b7968ac4869cb618":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6de3a1f8d6b948059b7c37ac9fa6174f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":6250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_99b6520761bb4a3daefdf1ee200331be"}},"783da47933514946a0df4f321a42bfc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_acad23a1eca9439788071dc64ca9be00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6250/6250 [46:36&lt;00:00,  2.23it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_612c8aa220d14974afb15872d66fb979"}},"6de3a1f8d6b948059b7c37ac9fa6174f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"99b6520761bb4a3daefdf1ee200331be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"acad23a1eca9439788071dc64ca9be00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"612c8aa220d14974afb15872d66fb979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0e8a2fe46db433798f3679f1f2d2501":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cd388012d1004b9a9a4f47910aca5f65","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33be911f5d68416d93c30b8106e24174","IPY_MODEL_8d0512d77717471eba56cb956be15f8e"]}},"cd388012d1004b9a9a4f47910aca5f65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33be911f5d68416d93c30b8106e24174":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2957e18c61bd476eb7923d486a9f23dd","_dom_classes":[],"description":"  1%","_model_name":"FloatProgressModel","bar_style":"","max":6250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":64,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e03832506aa4400f9f1c91a5199ec83e"}},"8d0512d77717471eba56cb956be15f8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01f5c0f834564256b87641debfbfa321","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 64/6250 [00:10&lt;16:28,  6.25it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c43d1448c0442f6954ef845379b6a61"}},"2957e18c61bd476eb7923d486a9f23dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e03832506aa4400f9f1c91a5199ec83e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01f5c0f834564256b87641debfbfa321":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1c43d1448c0442f6954ef845379b6a61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"39_7k4vJSFio","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"ok","timestamp":1595101546094,"user_tz":420,"elapsed":15490,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}},"outputId":"46ec1ef9-61a3-4690-9fad-569a169df012"},"source":["!pip install transformers\n","!pip install wget"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9iBqu3He1tSD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546095,"user_tz":420,"elapsed":1935,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["\n","from os import path\n","\n","inputfile=\"aclImdb_v1.tar.gz\"\n","\n","if not path.exists(inputfile) :\n","  !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","  !tar xvfz aclImdb_v1.tar.gz"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"gLSjueAc2nZa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546096,"user_tz":420,"elapsed":1915,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["#!ls aclImdb/test/neg/*"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SM0MWJgBoCu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595101546098,"user_tz":420,"elapsed":1901,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}},"outputId":"90d119e7-3133-44b4-f2f7-4a5e1e2afacf"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":75,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xrx2yKjfSbDi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546099,"user_tz":420,"elapsed":1854,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import BertForSequenceClassification,AdamW,BertTokenizer\n","from transformers import get_linear_schedule_with_warmup\n","from torchsummary import summary\n","\n","\n","from tqdm.auto import tqdm \n","import time\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn import metrics\n","from sklearn import model_selection\n","from sklearn.preprocessing import OneHotEncoder\n"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dr3-3_8s_cJy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546099,"user_tz":420,"elapsed":1827,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["MAX_LEN=512\n","TRAIN_BATCH_SIZE = 4\n","VALID_BATCH_SIZE = 4\n","EPOCHS = 3\n","MODEL_PATH=\"/content/gdrive/My Drive/bert_stanford_sent_anal_model.bin\"\n","OUTPUT_LOG=\"/content/gdrive/My Drive/bert_stanford_sent_anal_train.log\""],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEHun85ZlQki","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595101546100,"user_tz":420,"elapsed":1758,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}},"outputId":"83a3a650-b86f-4b05-e7e9-f70beafadae0"},"source":["if torch.cuda.is_available() :\n","  device = torch.device(\"cuda\")\n","  print('We will use the GPU:',torch.cuda.get_device_name(0))\n","else:\n","  print('No GPU available, using the CPU instead')\n","  device = torch.device(\"cpu\")"],"execution_count":78,"outputs":[{"output_type":"stream","text":["We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YWRGCIvGlKOg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595101546485,"user_tz":420,"elapsed":2100,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}},"outputId":"2ed7b683-0add-420a-92f1-a1d51355814b"},"source":["# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","enc = OneHotEncoder(handle_unknown='ignore')"],"execution_count":79,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bo3WLsqKWbUJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546486,"user_tz":420,"elapsed":2078,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["def log(value):\n","  if os.path.exists(OUTPUT_LOG) :\n","    f= open(OUTPUT_LOG,\"a\")\n","    f.write(value+\"\\n\")\n","    f.close()\n","  else:\n","    f= open(OUTPUT_LOG,\"w\")\n","    f.write(value+ \"\\n\")\n","    f.close()\n"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBBVaXWI4HUV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546487,"user_tz":420,"elapsed":2049,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["#Model\n","class BERTBaseUncased(nn.Module):\n","    def __init__(self):\n","        super(BERTBaseUncased, self).__init__()\n","        self.bert = BertForSequenceClassification.from_pretrained(\n","                      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","                      #num_labels = 2, # The number of output labels--2 for binary classification.\n","                      #output_attentions = False, # Whether the model returns attentions weights.\n","                      #output_hidden_states = False, # Whether the model returns all hidden-states.\n","                      )\n","    def forward(self, ids, mask, token_type_ids):\n","        o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n","        return o2[0]"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRPgA9NfouD9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546488,"user_tz":420,"elapsed":2026,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":[""],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsneONZY6cdI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546489,"user_tz":420,"elapsed":1981,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["class BERTDataset:\n","    def __init__(self, input_text, target):\n","        self.input_text = input_text\n","        self.target = target\n","        self.tokenizer = tokenizer\n","        self.max_len = MAX_LEN\n","\n","    def __len__(self):\n","        return len(self.input_text)\n","\n","    def __getitem__(self, item):\n","        input_text = str(self.input_text[item])\n","        input_text = \" \".join(input_text.split())\n","        \n","        inputs = self.tokenizer.encode_plus(\n","            input_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            truncation='longest_first'\n","        )\n","        \n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        return {\n","            \"ids\": torch.tensor(ids, dtype=torch.long),\n","            \"mask\": torch.tensor(mask, dtype=torch.long),\n","            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n","        }\n"],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_flTAfsEPXM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546490,"user_tz":420,"elapsed":1949,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":[""],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"id":"98cgTC6E4tun","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546490,"user_tz":420,"elapsed":1927,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["#Engine\n","\n","def loss_fn(outputs, targets):\n","    #print(\"OUTPUT\",outputs)\n","    return nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","\n","def train_fn(data_loader, model, optimizer, device, scheduler):\n","    model.train()\n","\n","    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","        \n","        ids = d[\"ids\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        mask = d[\"mask\"]\n","        targets = d[\"targets\"]\n","\n","        #print(\"ids\",ids.shape)\n","        #print(\"attn\",mask.shape)\n","        #print(\"token\",token_type_ids.shape)\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        optimizer.zero_grad()\n","        outputs = model(ids, mask, token_type_ids=token_type_ids)\n","\n","        loss = loss_fn(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        \n","\n","\n","def eval_fn(data_loader, model, device):\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            ids = d[\"ids\"]\n","            token_type_ids = d[\"token_type_ids\"]\n","            mask = d[\"mask\"]\n","            targets = d[\"targets\"]\n","\n","            ids = ids.to(device, dtype=torch.long)\n","            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","            mask = mask.to(device, dtype=torch.long)\n","            targets = targets.to(device, dtype=torch.float)\n","\n","            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","    return fin_outputs, fin_targets\n"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3z0eevOt2DA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546491,"user_tz":420,"elapsed":1888,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["def read_data(dir_path):\n","  classes=['pos','neg']\n","  values=[]\n","  targets=[]\n","  for cls in classes:\n","    base_path=dir_path + \"/\" + cls\n","    for entry in os.listdir(base_path):\n","      filename=os.path.join(base_path, entry)\n","      if os.path.isfile(filename):\n","        with open(filename, 'r') as file:\n","          data = file.read()\n","          values.append(data)\n","          targets.append(cls)\n","  #enc.fit(targets.reshape(-1,1))\n","  \n","  targets=enc.fit_transform(np.array(targets).reshape(-1,1)).toarray()\n","  print(targets.shape)\n","  return values,targets\n"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SzNSCebRNyZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546492,"user_tz":420,"elapsed":1871,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["def show_tokens(sentence):\n","    max_len = MAX_LEN\n","    input_text = str(sentence)\n","    input_text = \" \".join(input_text.split())\n","\n","  \n","    inputs = tokenizer.encode_plus(\n","            input_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=max_len,\n","            pad_to_max_length=True,\n","            truncation='longest_first'\n","        )\n","\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","    token_type_ids = inputs[\"token_type_ids\"]\n","    print(ids)\n","    print(mask)\n","    print(token_type_ids)\n","\n","#show_tokens(\"Nice  picturization of the song on the beaches of Tahiti.[SEP]Very good acting by Abhishek and Aishwarya. \")\n","#show_tokens(\"Good Movie\")"],"execution_count":85,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOoe4Fgv8ds8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546492,"user_tz":420,"elapsed":1848,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["#Training\n","\n","import random\n","\n","def train():\n"," \n","    train_path='aclImdb/train'\n","    test_path='aclImdb/test'\n","\n","    train_values,train_targets=read_data(train_path)\n","    test_values,test_targets=read_data(test_path)\n","\n","    #for i in range(0,10) :\n","      #print(train_targets[i],train_values[i])\n","\n","    print(len(train_targets))\n","    print(len(train_values))\n","\n","    print(len(test_targets))\n","    print(len(test_values))\n","\n","    choices = list(range(len(train_values)))\n","    random.shuffle(choices)\n","\n","    new_train_values = []\n","    new_train_targets = []\n","\n","    for n in choices :\n","      new_train_values.append(train_values[choices[n]])\n","      new_train_targets.append(train_targets[choices[n]])\n","\n","    train_dataset = BERTDataset(        \n","        input_text=new_train_values, target=new_train_targets\n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4,\n","    )\n","\n","    valid_dataset = BERTDataset(\n","        input_text=test_values, target=test_targets\n","    )\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1,\n","    )\n","\n","    log(\"Train Size=\"+str(len(train_dataset)))\n","    log(\"Valid size=\"+str(len(valid_dataset)))\n","    \n","    model = BERTBaseUncased()\n","    model.to(device)\n","    #print(model)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {\n","            \"params\": [\n","                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.001,\n","        },\n","        {\n","            \"params\": [\n","                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","\n","    num_train_steps = int(len(train_targets) / TRAIN_BATCH_SIZE * EPOCHS)\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n","    )\n","\n","    model = nn.DataParallel(model)\n","\n","    best_accuracy = 0\n","    for epoch in range(EPOCHS):\n","        train_fn(train_data_loader, model, optimizer, device, scheduler)\n","        print(\"Calling eval\")\n","        outputs, targets = eval_fn(valid_data_loader, model, device)\n","        outputs = np.array(outputs) >= 0.5\n","        accuracy = metrics.accuracy_score(targets, outputs)\n","        log(\" Epoch=\"+str(epoch)+\"Accuracy=\"+str(accuracy))\n","        print(f\"Accuracy Score = {accuracy}\")\n","        if accuracy > best_accuracy:\n","            torch.save(model.state_dict(), MODEL_PATH)\n","            best_accuracy = accuracy\n","    return model"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDEL-VdeB_Yk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546494,"user_tz":420,"elapsed":1823,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["#Predict\n","PREDICTION_CACHE = dict()\n","\n","def predict_from_cache(model,sentence):\n","    if sentence in PREDICTION_DICT:\n","        return PREDICTION_DICT[sentence]\n","    else:\n","        result = sentence_prediction(model,sentence)\n","        PREDICTION_CACHE[sentence] = result\n","        return result\n","\n","def sentence_prediction(model,sentence):\n","    max_len = MAX_LEN\n","    input_text = str(sentence)\n","    input_text = \" \".join(input_text.split())\n","\n","  \n","    inputs = tokenizer.encode_plus(\n","            input_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=max_len,\n","            #pad_to_max_length=True,\n","            truncation='longest_first'\n","        )\n","\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","    token_type_ids = inputs[\"token_type_ids\"]\n","\n","    padding_length = max_len - len(ids)\n","    ids = ids + ([0] * padding_length)\n","    mask = mask + ([0] * padding_length)\n","    token_type_ids = token_type_ids + ([0] * padding_length)\n","\n","    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n","    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n","    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n","\n","    ids = ids.to(device, dtype=torch.long)\n","    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","    mask = mask.to(device, dtype=torch.long)\n","    \n","    outputs = model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n","    outputs = torch.sigmoid(outputs[0]).cpu().detach().numpy()\n","\n","    cls = np.round(outputs[0].reshape(-1,2))\n","    return enc.inverse_transform(cls)"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCvF8cLWDDEB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595101546494,"user_tz":420,"elapsed":1798,"user":{"displayName":"Damodar Hegde","photoUrl":"","userId":"10146457441057083367"}}},"source":["def predict() :\n","  \n","  model_state_dict=torch.load(MODEL_PATH)\n","  model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",state_dict=model_state_dict)\n","  model.to(device)\n","    \n","  out = sentence_prediction(model,\"Sholay is a good movie, I like Thanku's acting very much\")\n","  print(out)\n","  out = sentence_prediction(model,\"Good Movie\")\n","  print(out)\n","  out= sentence_prediction(model,\"Uski Roti was a boring movie, there is no drama , just the routine visits to home by husband etc.\")\n","  print(out)\n","  out= sentence_prediction(model,\"bad movie\")\n","  print(out)"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"id":"ET2CA8_YPZlk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":327,"referenced_widgets":["a8aeb07b4b33433aa170cf7478e1fbae","bd0ea0b9634346e380e2888dfd00c942","77e86825addb40c9b7968ac4869cb618","783da47933514946a0df4f321a42bfc2","6de3a1f8d6b948059b7c37ac9fa6174f","99b6520761bb4a3daefdf1ee200331be","acad23a1eca9439788071dc64ca9be00","612c8aa220d14974afb15872d66fb979","b0e8a2fe46db433798f3679f1f2d2501","cd388012d1004b9a9a4f47910aca5f65","33be911f5d68416d93c30b8106e24174","8d0512d77717471eba56cb956be15f8e","2957e18c61bd476eb7923d486a9f23dd","e03832506aa4400f9f1c91a5199ec83e","01f5c0f834564256b87641debfbfa321","1c43d1448c0442f6954ef845379b6a61"]},"outputId":"95543470-5d93-403c-8fb0-03d569f34bc9"},"source":["train()\n","predict()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(25000, 2)\n","(25000, 2)\n","25000\n","25000\n","25000\n","25000\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8aeb07b4b33433aa170cf7478e1fbae","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6250.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Calling eval\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0e8a2fe46db433798f3679f1f2d2501","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6250.0), HTML(value='')))"]},"metadata":{"tags":[]}}]}]}